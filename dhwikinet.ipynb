{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjN4+ovo+Sq4nafiRFpXDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomouvic/txtanalytics/blob/main/dhwikinet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a network of Wikipedia pages starting from a given page (e.g. Digital Humanities) and collecting its ego-net. \n",
        "\n",
        "Adapted from: Complex Network Analysis in Python\n",
        "Recognize → Construct → Visualize → Analyze → Interpret\n",
        "by Dmitry Zinoviev."
      ],
      "metadata": {
        "id": "sCrklToqkqE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNmFbnlalMqN",
        "outputId": "537e4828-6e05-4909-dc1d-17e497d598de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=c19d35d77709db8014d9375ef1a1e8783aaff5b35add15007b1043bc32d6136a\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6nCF3OlbkdMo"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "import networkx as nx\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SEED = \"Complex network\".title()\n",
        "SEED = \"Digital humanities\".title()\n",
        "\n",
        "STOPS = (\"International Standard Serial Number\",\n",
        "         \"International Standard Book Number\",\n",
        "         \"National Diet Library\",\n",
        "         \"International Standard Name Identifier\",\n",
        "         \"International Standard Book Number (Identifier)\",\n",
        "         \"Pubmed Identifier\", \"Pubmed Central\",\n",
        "         \"Digital Object Identifier\", \"Arxiv\",\n",
        "         \"Proc Natl Acad Sci Usa\", \"Bibcode\",\n",
        "         \"Library Of Congress Control Number\", \"Jstor\", \n",
        "         \"ISBN\", \"Doi\", \"Issn\")\n",
        "\n",
        "todo_lst = [(0, SEED)] # The SEED is in the layer 0\n",
        "todo_set = set(SEED)   # The SEED itself\n",
        "done_set = set()       # Nothing is done yet\n",
        "\n",
        "\n",
        "\n",
        "F = nx.DiGraph()\n",
        "layer, page = todo_lst[0]\n",
        "\n",
        "while layer < 2:\n",
        "    del todo_lst[0] #(1)\n",
        "    done_set.add(page)\n",
        "    print(layer, page) # Show progress\n",
        "\n",
        "    try: #(2)\n",
        "        wiki = wikipedia.page(page)\n",
        "    except:\n",
        "        layer, page = todo_lst[0]\n",
        "        print(\"Could not load\", page)\n",
        "        continue\n",
        "\n",
        "    for link in wiki.links: #(3)\n",
        "        link = link.title()\n",
        "        if link not in STOPS and not link.startswith(\"List Of\"):\n",
        "            if link not in todo_set and link not in done_set:\n",
        "                todo_lst.append((layer + 1, link))\n",
        "                todo_set.add(link)\n",
        "            F.add_edge(page, link)\n",
        "\n",
        "    layer, page = todo_lst[0] #(4)\n",
        "print(\"{} nodes, {} edges\".format(len(F), nx.number_of_edges(F)))\n",
        "# 11597 nodes, 21331 edges\n",
        "\n",
        "# Alex: 14300 nodes, 28192 edges (for Complex Network)\n",
        "# Alex: 34385 nodes, 63182 edges (for Digital Humanities)"
      ],
      "metadata": {
        "id": "QN0zxW3MmkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F.remove_edges_from(F.selfloop_edges())\n",
        "F.remove_edges_from(nx.selfloop_edges(F, data=True))\n",
        "duplicates = [(node, node + \"s\") for node in F if node + \"s\" in F]\n",
        "for dup in duplicates:\n",
        "    F = nx.contracted_nodes(F, *dup, self_loops=False)\n",
        "duplicates = [(x, y) for x, y \n",
        "              in [(node, node.replace(\"-\", \" \")) for node in F]\n",
        "              if x != y and y in F]\n",
        "for dup in duplicates:\n",
        "    F = nx.contracted_nodes(F, *dup, self_loops=False)\n",
        "nx.set_node_attributes(F, 0, \"contraction\")"
      ],
      "metadata": {
        "id": "AgXadcM9tpwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A: remove some useless high degree nodes\n",
        "useless = ['Doi (Identifier)', 'Isbn (Identifier)', 'Issn (Identifier)', 'Internet', 'Html', 'Google']\n",
        "for x in useless:\n",
        "  if F.has_node(x):\n",
        "    F.remove_node(x)\n",
        "\n",
        "core = [node for node, deg in dict(F.degree()).items() if deg >= 2]\n",
        "G = nx.subgraph(F, core)\n",
        "\n",
        "print(\"{} nodes, {} edges\".format(len(G), nx.number_of_edges(G)))\n",
        "\n",
        "# nx.write_graphml(G, \"cna.graphml\")\n",
        "\n",
        "top_indegree = sorted(dict(G.in_degree()).items(),\n",
        "                      reverse=True, key=itemgetter(1))[:200]\n",
        "\n",
        "\n",
        "# print(top_indegree)\n",
        "for x in top_indegree: \n",
        "  print(x[1], x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjmiSZx4Iyp",
        "outputId": "3df2906f-e0c4-4d79-b03b-a1a833a3ba35"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8076 nodes, 34378 edges\n",
            "119 Digital Humanities\n",
            "110 S2Cid (Identifier)\n",
            "101 Digital Library\n",
            "96 Semantic Web\n",
            "87 Database\n",
            "84 World Wide Web\n",
            "84 Metadata\n",
            "80 Semantic Network\n",
            "79 Ontology (Information Science)\n",
            "77 Knowledge Representation And Reasoning\n",
            "76 Hypertext\n",
            "76 Knowledge Management\n",
            "75 Knowledge Extraction\n",
            "75 Semantics (Computer Science)\n",
            "75 Xml\n",
            "75 Information Architecture\n",
            "75 Web 2.0\n",
            "75 Simple Knowledge Organization System\n",
            "74 Collective Intelligence\n",
            "73 Semantic Reasoner\n",
            "73 Semantic Publishing\n",
            "73 Hypertext Transfer Protocol\n",
            "73 Dublin Core\n",
            "72 Shacl\n",
            "72 Semantic Computing\n",
            "72 Doap\n",
            "72 Resource Description Framework\n",
            "72 Json-Ld\n",
            "72 Rdfa\n",
            "72 Schema.Org\n",
            "72 Internationalized Resource Identifier\n",
            "72 Web Ontology Language\n",
            "72 Linked Data\n",
            "72 Notation3\n",
            "72 Sparql\n",
            "72 Topic Map\n",
            "72 Uniform Resource Identifier\n",
            "72 Foaf\n",
            "71 N-Triples\n",
            "71 Semantic Matching\n",
            "71 Hyperdata\n",
            "71 Common Logic\n",
            "71 Semantic Analytics\n",
            "71 Semantic Html\n",
            "71 Rule Interchange Format\n",
            "71 Trig (Syntax)\n",
            "71 Grddl\n",
            "71 Microdata (Html)\n",
            "71 Hreview\n",
            "71 Rdf Schema\n",
            "71 Description Logic\n",
            "71 Hproduct\n",
            "71 Web Science Trust\n",
            "71 Dataspaces\n",
            "71 Semantic Mapper\n",
            "71 Semantic Triple\n",
            "71 Hcalendar\n",
            "71 Geotagging\n",
            "71 Application-Level Profile Semantics (Alps)\n",
            "71 Folksonomy\n",
            "71 Turtle (Syntax)\n",
            "71 Library 2.0\n",
            "71 Research Resource Identifier\n",
            "71 Semantic Service-Oriented Architecture\n",
            "71 Semantically-Interlinked Online Communities\n",
            "71 Solid (Web Decentralization Project)\n",
            "71 Trix (Serialization Format)\n",
            "71 Semantic Search\n",
            "71 Hatom\n",
            "71 Microformat\n",
            "71 Rdf/Xml\n",
            "71 Reference (Computer Science)\n",
            "71 Embedded Rdf\n",
            "71 Semantic Web Rule Language\n",
            "71 Hcard\n",
            "71 Web Engineering\n",
            "71 Rule-Based System\n",
            "71 Semantic Broker\n",
            "71 Sawsdl\n",
            "71 Facebook Platform\n",
            "71 Hrecipe\n",
            "71 Semantic Wiki\n",
            "66 Wayback Machine\n",
            "59 Oclc (Identifier)\n",
            "58 Pmid (Identifier)\n",
            "45 Computer Science\n",
            "40 Pmc (Identifier)\n",
            "39 Jstor (Identifier)\n",
            "39 Bibcode (Identifier)\n",
            "37 Viaf (Identifier)\n",
            "35 New Media\n",
            "29 Citeseerx (Identifier)\n",
            "29 Hdl (Identifier)\n",
            "29 Isni (Identifier)\n",
            "28 World Wide Web Consortium\n",
            "28 Sudoc (Identifier)\n",
            "27 Linguistics\n",
            "26 Semantic\n",
            "26 The New York Times\n",
            "26 Artificial Intelligence\n",
            "25 Philosophy\n",
            "25 Digitization\n",
            "25 History\n",
            "25 Law\n",
            "25 Text Encoding Initiative\n",
            "24 Philosophy Of Computer Science\n",
            "24 Systems Theory\n",
            "24 Cybertext\n",
            "24 W3C\n",
            "24 Electronic Literature\n",
            "24 Digital Rhetoric\n",
            "24 Sociology\n",
            "24 Anthropology\n",
            "23 Cultural Analytics\n",
            "23 Wikipedia\n",
            "23 Information Science\n",
            "23 Humanities\n",
            "23 Computational Theory Of Mind\n",
            "23 Digital Physics\n",
            "22 Computational Philosophy\n",
            "22 Algorithm\n",
            "22 Computational Archaeology\n",
            "22 Digital History\n",
            "22 Digital Ontology\n",
            "22 Open-Access\n",
            "22 United States\n",
            "22 Social Science\n",
            "22 Arxiv (Identifier)\n",
            "21 Digital Medievalist\n",
            "21 Computers And Writing\n",
            "21 Economics\n",
            "21 Data Mining\n",
            "21 Transliteracy\n",
            "21 E-Research\n",
            "21 Digital Religion\n",
            "21 Digital Classics\n",
            "21 Digital Theology\n",
            "21 Digital Scholarship\n",
            "21 Humanistic Informatics\n",
            "20 Machine Learning\n",
            "20 Social Media\n",
            "20 Science And Technology Studies\n",
            "20 Cultural Studies\n",
            "20 Political Science\n",
            "20 Web Browser\n",
            "19 Wikidata\n",
            "19 Open-Source Software\n",
            "19 Psychology\n",
            "19 Communication Studies\n",
            "19 Philosophy Of Science\n",
            "19 Philosophy Of Social Science\n",
            "19 Natural Language Processing\n",
            "18 Language\n",
            "18 Archaeology\n",
            "18 Statistic\n",
            "18 Information Retrieval\n",
            "18 Cognitive Science\n",
            "18 Tim Berners-Lee\n",
            "18 Communication\n",
            "18 Information\n",
            "17 Software\n",
            "17 Html5\n",
            "17 Aristotle\n",
            "17 Ontology\n",
            "17 Geography\n",
            "17 Logic\n",
            "16 History Of Science\n",
            "16 Media Studies\n",
            "16 Science Studies\n",
            "16 Geographic Information System\n",
            "16 Culture\n",
            "16 Philosophy And Economics\n",
            "16 Youtube\n",
            "16 Political Philosophy\n",
            "16 Demography\n",
            "16 Cultural Anthropology\n",
            "16 Ibm\n",
            "15 Concept\n",
            "15 History Of Technology\n",
            "15 Sociology Of The Internet\n",
            "15 Political Sociology\n",
            "15 Science\n",
            "15 Latin\n",
            "15 Literature\n",
            "15 Politic\n",
            "15 Education\n",
            "15 Social Psychology\n",
            "15 Xhtml\n",
            "15 Cognitive Psychology\n",
            "15 Philosophy Of Psychology\n",
            "15 Knowledge Base\n",
            "15 Philosophy Of History\n",
            "15 Hyperlink\n",
            "15 Political Economy\n",
            "15 Facebook\n",
            "15 Historical Sociology\n",
            "15 Massachusetts Institute Of Technology\n",
            "14 Social Anthropology\n",
            "14 Political Ecology\n",
            "14 Intellectual Property\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.write_gexf(G, \"dh_wikipedia.gexf\")"
      ],
      "metadata": {
        "id": "goanDRBGADYL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wiki = wikipedia.page('Isbn (Identifier)')\n",
        "# print(wiki)\n",
        "# print(wiki.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yErTkQZd1deh",
        "outputId": "8fef61bd-58a9-4d0e-f55b-b745a58c5ba3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<WikipediaPage 'ISBN'>\n",
            "ISBN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for node in G.nodes:\n",
        "#   for neighbor in G.successors(node):\n",
        "#     print(node, ',', neighbor)"
      ],
      "metadata": {
        "id": "irif536L8JKE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HQbdoKA6aj0",
        "outputId": "3fa4109b-a7b4-492c-d2a3-710b2fb3d56e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 8394 nodes and 37263 edges\n"
          ]
        }
      ]
    }
  ]
}